{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# L'API d'OpenAI - https://platform.openai.com/\n",
    "\n",
    "L’API d'OpenAI permet aux développeurs d'accéder aux modèles de traitement du langage naturel GPT-3 de OpenAI pour diverses applications, allant de la génération de contenu à la création d'applications de chatbot.\n",
    "\n",
    "\n",
    "<img src='https://images.lifestyleasia.com/wp-content/uploads/sites/6/2023/01/30023721/openai-chatgpt-elon-musk-artificial-intelligence.png?tr=w-1200,h-900'>\n",
    "\n",
    "A travers [la plateforme d'OpenAI](https://platform.openai.com/), les développeurs ont accès à :\n",
    "- La page de bienvenue présentant les fonctionnalité principales de l’API.\n",
    "- La page de documentation qui présente les principe technique de fonctionnement de l’API via des tutoriels.\n",
    "La page API reference qui présente les point de terminaison de l’API.\n",
    "- La page Exemple qui présente des cas d’utilisation de l’API.\n",
    "- La page Playground qui permet de tester les modèles de génération de texte."
   ],
   "metadata": {
    "id": "8bqu-La-5EFU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 1. Les modèles GPT\n",
    "Le modèle GPT (Generative Pre-trained Transformer) est un modèle de traitement du langage naturel (NLP) basé sur la famille des transformaters. Il a été développé par OpenAI et a été publié pour la première fois en 2018.\n",
    "\n",
    "Le modèle GPT est un modèle de langage pré-entraîné, ce qui signifie qu'il a été entraîné sur un grand corpus de texte non annoté avant d'être fine-tuné pour des tâches spécifiques. GPT a été entraîné sur un corpus de textes en anglais provenant de sources variées telles que des pages web, des livres et des articles de journaux. Il utilise une architecture transformer, qui utilise des couches d'auto-attention pour encoder la séquence d'entrée.\n",
    "\n",
    "* **GPT-1**, la première version de GPT, a été entraîné sur un corpus de 40 Go de texte.\n",
    "\n",
    "* **GPT-2**, la deuxième version de GPT, a été entraîné sur un corpus de 1,5 To de texte et est devenu célèbre pour sa capacité à générer des textes de qualité proche de celle de l'homme.\n",
    "\n",
    "* **GPT-3**, la troisième version de GPT, est l'un des plus grands modèles de langage pré-entraîné au monde, avec 175 milliards de paramètres, et a démontré une capacité impressionnante à effectuer une large gamme de tâches de NLP sans fine-tuning important.\n",
    "\n",
    "* **GPT-4**, la quatrième itération de la série GPT, représente une avancée significative dans le domaine de l'apprentissage automatique et du traitement du langage naturel avec près de 100 000 milliards de paramètres.\n",
    "\n",
    "**L'importance du `prompt`**\n",
    "\n",
    "Un prompt est un texte court qui est utilisé pour initier une tâche ou une interaction avec un système d'IA (intelligence artificielle). Dans le domaine du traitement du langage naturel (NLP), les prompts sont souvent utilisés pour guider la génération de texte ou pour interroger un système de question-réponse.\n",
    "\n",
    "Par exemple, dans le cadre d'une conversation avec un chatbot, le prompt peut être une question ou une déclaration qui invite le chatbot à fournir une réponse appropriée. Dans le cadre de la génération de texte, le prompt peut être une phrase ou un paragraphe qui sert de point de départ à un modèle de langage pour générer du texte cohérent.\n",
    "\n",
    "Les prompts sont devenus de plus en plus populaires avec l'avènement des GPT (Generative Pre-trained Transformer) qui sont des modèles de NLP pré-entraînés capables de générer du texte de manière autonome. Les utilisateurs peuvent fournir des prompts à ces modèles pour générer une variété de contenus, y compris des histoires, des articles de blog, des poèmes et même du code informatique."
   ],
   "metadata": {
    "id": "OHNpLB4ct0z3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlRFn2_i5DUy",
    "outputId": "1d83db74-2545-476d-e158-84279cae55d1",
    "ExecuteTime": {
     "end_time": "2023-12-14T08:49:42.797171700Z",
     "start_time": "2023-12-14T08:49:32.704474100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28.0\n",
      "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\thomas.caron\\projects\\webscraping\\venv\\lib\\site-packages (from openai==0.28.0) (2.31.0)\n",
      "Collecting tqdm (from openai==0.28.0)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting aiohttp (from openai==0.28.0)\n",
      "  Downloading aiohttp-3.9.1-cp39-cp39-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thomas.caron\\projects\\webscraping\\venv\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thomas.caron\\projects\\webscraping\\venv\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thomas.caron\\projects\\webscraping\\venv\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thomas.caron\\projects\\webscraping\\venv\\lib\\site-packages (from requests>=2.20->openai==0.28.0) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thomas.caron\\projects\\webscraping\\venv\\lib\\site-packages (from aiohttp->openai==0.28.0) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->openai==0.28.0)\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->openai==0.28.0)\n",
      "  Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl.metadata (32 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->openai==0.28.0)\n",
      "  Downloading frozenlist-1.4.0-cp39-cp39-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->openai==0.28.0)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->openai==0.28.0)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\thomas.caron\\projects\\webscraping\\venv\\lib\\site-packages (from tqdm->openai==0.28.0) (0.4.6)\n",
      "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.5/76.5 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.1-cp39-cp39-win_amd64.whl (365 kB)\n",
      "   ---------------------------------------- 0.0/365.5 kB ? eta -:--:--\n",
      "   -------------------- ------------------ 194.6/365.5 kB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 365.5/365.5 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp39-cp39-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.7/44.7 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.9/76.9 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.0 multidict-6.0.4 openai-0.28.0 tqdm-4.66.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28.0"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import openai\n",
    "#Placer votre clé API dans la variable ci-dessous\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "[model['id'] for model in openai.Model().list()[\"data\"]]"
   ],
   "metadata": {
    "id": "Yv61WSlB5glS",
    "ExecuteTime": {
     "end_time": "2023-12-14T08:50:51.970666400Z",
     "start_time": "2023-12-14T08:50:51.183041600Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['text-search-babbage-doc-001',\n 'curie-search-query',\n 'text-davinci-003',\n 'text-search-babbage-query-001',\n 'babbage',\n 'babbage-search-query',\n 'text-babbage-001',\n 'text-similarity-davinci-001',\n 'davinci-similarity',\n 'code-davinci-edit-001',\n 'curie-similarity',\n 'babbage-search-document',\n 'curie-instruct-beta',\n 'text-search-ada-doc-001',\n 'davinci-instruct-beta',\n 'whisper-1',\n 'text-similarity-babbage-001',\n 'text-search-davinci-doc-001',\n 'gpt-3.5-turbo-16k-0613',\n 'gpt-3.5-turbo-16k',\n 'babbage-similarity',\n 'text-embedding-ada-002',\n 'davinci-search-query',\n 'text-similarity-curie-001',\n 'text-davinci-001',\n 'text-search-davinci-query-001',\n 'ada-search-document',\n 'ada-code-search-code',\n 'babbage-002',\n 'davinci-002',\n 'davinci-search-document',\n 'curie-search-document',\n 'gpt-4-0613',\n 'babbage-code-search-code',\n 'text-search-ada-query-001',\n 'code-search-ada-text-001',\n 'babbage-code-search-text',\n 'gpt-4-vision-preview',\n 'code-search-babbage-code-001',\n 'ada-search-query',\n 'gpt-3.5-turbo',\n 'ada-code-search-text',\n 'tts-1-hd',\n 'text-search-curie-query-001',\n 'text-davinci-002',\n 'text-davinci-edit-001',\n 'code-search-babbage-text-001',\n 'ada',\n 'text-ada-001',\n 'ada-similarity',\n 'code-search-ada-code-001',\n 'text-similarity-ada-001',\n 'gpt-3.5-turbo-0301',\n 'gpt-4',\n 'text-search-curie-doc-001',\n 'text-curie-001',\n 'curie',\n 'tts-1',\n 'gpt-4-0314',\n 'davinci',\n 'gpt-3.5-turbo-1106',\n 'gpt-3.5-turbo-instruct-0914',\n 'gpt-4-1106-preview',\n 'dall-e-2',\n 'gpt-3.5-turbo-instruct',\n 'gpt-3.5-turbo-0613',\n 'tts-1-1106',\n 'tts-1-hd-1106',\n 'dall-e-3']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#openai.Model().list()"
   ],
   "metadata": {
    "id": "Gtc2UG3j5h5B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Text Completion\n",
    "\n",
    "La complétion de texte est une tâche de NLP qui consiste à prédire la suite d'un texte donné. Par exemple, si nous donnons le texte \"Je suis un étudiant en informatique\", le modèle doit prédire la suite du texte, qui est \"à l'Université de Lomé\"."
   ],
   "metadata": {
    "id": "yHc_aaLSuaxb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Générateur de texte\n",
    "response = openai.Completion.create(\n",
    "    engine=\"davinci\",\n",
    "    prompt=\"Je suis un chatbot concu pour\",\n",
    "    temperature=1,\n",
    "    max_tokens=300)\n",
    "\n",
    "response.choices[0].text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "96Qpkwzv5h2j",
    "outputId": "2939295e-e025-49b0-9e4a-c23e134b38ee",
    "ExecuteTime": {
     "end_time": "2023-12-14T08:54:28.421594300Z",
     "start_time": "2023-12-14T08:54:19.530778800Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "' gener une salle de classe virtuel car ici la salle de clase est tres memees Le nombre de problemes se multiplie Autodata classeur breaks simone chazal badre wbc THE IE CREATES THE INTERNET ESSAY Essay on internet freedom iieworld india internet essay in It’s one of the results of interacting with the Internet Essays on the Internet in China and the attempt to set up a Great Firewall Von Andreas Speit\\n\\nThe Anarchic Internet Essays on Cyberspace Technology, and the\\n\\nDit is de digitale faculteit Pull this notice once closed In this example, a more long term goal is to ensure they continue to be qualified for work in their chosen profession It is open source and uses a block chain to store Markdown files If you would like me to continue work for you, simply place another order You will see a unique URL linked to\\n\\nThe Internet and Society an Ethnographic Approach\\n\\nWe will vet every enrolled Dollarama candidate for documents movement aaron kranz j7e wco jeuphoria death of a salesman global vision tv series 720p jeuphoria online partnervermittlung wien cryptocurrency cheap mlb jerseys espn 88 hair color 11 eyesight convertibles A beverage is a noncarbonated food served in its own container your human understanding has years of treatment experience Brain decoded vascular malformation W Dien'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Traduction avec ChatGPT\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Translate this into 1. French, 2. Spanish and 3. Japanese:\\n\\nWhat rooms do you have available?\\n\\n1.\",\n",
    "  temperature=1,\n",
    "  max_tokens=100,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "response.choices[0].text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-AfuaAez5hzj",
    "outputId": "e37f175a-678f-4ba4-c02d-e102dd2da750",
    "ExecuteTime": {
     "end_time": "2023-12-14T08:55:31.716627Z",
     "start_time": "2023-12-14T08:55:30.138874100Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "' Quelle chambres avez-vous disponible?\\n2. ¿Qué habitaciones tienes disponibles?\\n3. どの部屋が利用可能ですか？'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 ChatCompletion\n",
    "\n",
    "Les rôles du model GPT-3 dans le chatbot sont les suivants :\n",
    "- **system**: le chatbot, on donne ici le role de system au model GPT-3\n",
    "- **user**: l'utilisateur, on fournira ici la réponse de l'utilisateur\n",
    "- **agent**: l'agent de l'API d'OpenAI, on fournira ici la réponse de l'agent"
   ],
   "metadata": {
    "id": "X_SkQbWEuhVu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "query = input('What do you want to search for ? ')\n",
    "text = requests.get(f'https://www.bing.com/news/search?q={query}').text\n",
    "soup = bs4.BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "actu = ' '.join([\"- Actualité : \" + link.text+ ' \\n'  for link in soup.find_all('a', 'title')])\n",
    "actu"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "cnZ3gnNc5hwc",
    "outputId": "d18edb01-47b1-4b6c-930f-fe86da8eaa14",
    "ExecuteTime": {
     "end_time": "2023-12-14T08:56:40.867855200Z",
     "start_time": "2023-12-14T08:56:33.305484100Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "'- Actualité : Jordan Bardella s\\'installe parmi les dix personnalités politiques les plus populaires \\n - Actualité : Jordan Bardella se hisse dans le top 10 des personnalités politiques préférées des Français et dépasse Marine Le Pen \\n - Actualité : Jordan Bardella dépasse Marine Le Pen dans un sondage \\n - Actualité : En cas de dissolution, Jordan Bardella accepterait d\\'être un Premier ministre de cohabitation \\n - Actualité : Sondage : Jordan Bardella désormais plus populaire que… Marine Le Pen \\n - Actualité : \"Vous m\\'agressez depuis 9 minutes\": échange tendu avec Jordan Bardella, une journaliste écartée \\n - Actualité : Loi immigration: Jordan Bardella réclame une dissolution et se dit \"prêt\" à être Premier ministre \\n - Actualité : Après son interview de Jordan Bardella, Barbara Olivier-Zandronis réagit : «Ce n’était pas du militantisme, mais du journalisme» \\n - Actualité : Jordan Bardella se dit prêt à être Premier ministre en cas de dissolution de l\\'Assemblée \\n - Actualité : \"La seule voie honorable\": Jordan Bardella appelle à la dissolution de l\\'Assemblée nationale \\n - Actualité : Jordan Bardella demande la dissolution de l\\'Assemblée nationale \\n - Actualité : Sondage : Jordan Bardella dépasse Marine Le Pen \\n'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "actu.split('\\n')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8ps-yu05htl",
    "outputId": "85c6e32d-caf1-4f68-cae9-d24910fa452e",
    "ExecuteTime": {
     "end_time": "2023-12-14T08:56:48.163216800Z",
     "start_time": "2023-12-14T08:56:48.116143400Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[\"- Actualité : Jordan Bardella s'installe parmi les dix personnalités politiques les plus populaires \",\n ' - Actualité : Jordan Bardella se hisse dans le top 10 des personnalités politiques préférées des Français et dépasse Marine Le Pen ',\n ' - Actualité : Jordan Bardella dépasse Marine Le Pen dans un sondage ',\n \" - Actualité : En cas de dissolution, Jordan Bardella accepterait d'être un Premier ministre de cohabitation \",\n ' - Actualité : Sondage : Jordan Bardella désormais plus populaire que… Marine Le Pen ',\n ' - Actualité : \"Vous m\\'agressez depuis 9 minutes\": échange tendu avec Jordan Bardella, une journaliste écartée ',\n ' - Actualité : Loi immigration: Jordan Bardella réclame une dissolution et se dit \"prêt\" à être Premier ministre ',\n ' - Actualité : Après son interview de Jordan Bardella, Barbara Olivier-Zandronis réagit : «Ce n’était pas du militantisme, mais du journalisme» ',\n \" - Actualité : Jordan Bardella se dit prêt à être Premier ministre en cas de dissolution de l'Assemblée \",\n ' - Actualité : \"La seule voie honorable\": Jordan Bardella appelle à la dissolution de l\\'Assemblée nationale ',\n \" - Actualité : Jordan Bardella demande la dissolution de l'Assemblée nationale \",\n ' - Actualité : Sondage : Jordan Bardella dépasse Marine Le Pen ',\n '']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "reponse = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": f\"Tu es un rédacteur web qui synthétise l'actualité en 50 mots sur la thématique '{query}' Tu fais des liaisons entre les articles avec des mots tel que 'mais', 'donc', 'or', 'par contre', 'en revanche', 'en effet', 'cependant', 'toutefois', 'par ailleurs', 'par contre', 'par contre, 'enfin'\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"Voici la liste des actualités à synthétiser :\" + actu},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "reponse['choices'][0]['message'][\"content\"]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "3OY1YYtx5hpq",
    "outputId": "0929d69d-a6e4-4c65-e68b-4fde866b7304",
    "ExecuteTime": {
     "end_time": "2023-12-14T08:57:14.977474700Z",
     "start_time": "2023-12-14T08:57:09.134514300Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'Jordan Bardella s\\'installe parmi les dix personnalités politiques les plus populaires, dépassant même Marine Le Pen dans un sondage. Il se dit prêt à être Premier ministre en cas de dissolution de l\\'Assemblée et réclame cette dissolution en évoquant \"la seule voie honorable\". Cependant, il est critiqué pour ses échanges tendus avec les journalistes.'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **2. DALL-E et de l'API DALL-E d'OpenAI en Python**\n",
    "\n",
    "<img src='https://images.openai.com/blob/d3b684dc-eb13-4fbd-a77e-0a397e059fef/dall-e-1.jpg?trim=0,0,0,0&width=3200'>\n",
    "\n",
    "Dans ce notebook, nous allons explorer l'utilisation de DALL-E, une puissante modèle de génération d'images développé par OpenAI. DALL-E est capable de générer des images à partir de descriptions textuelles, ou de transformer des images existantes en fonction de certaines spécifications.\n",
    "\n",
    "Nous allons également voir comment utiliser l'API DALL-E d'OpenAI pour intégrer cette technologie dans nos applications Python.\n",
    "\n",
    "\n",
    "## 2.1 Création d'images à partir de texte\n",
    "\n",
    "DALL-E est un modèle de génération d'images qui peut créer des images à partir de descriptions textuelles. Il est capable de générer des images à partir de descriptions textuelles, ou de transformer des images existantes en fonction de certaines spécifications."
   ],
   "metadata": {
    "id": "i3BjNZnusjGk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Configurations & Imports\n",
    "from IPython.display import Image\n",
    "import requests\n",
    "\n",
    "# Generate image from text\n",
    "import openai\n",
    "\n",
    "openai.api_key = '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'"
   ],
   "metadata": {
    "id": "cbDmatg_rh1j",
    "ExecuteTime": {
     "end_time": "2023-12-14T08:58:32.114188200Z",
     "start_time": "2023-12-14T08:58:32.067501600Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Generate image from text\n",
    "response = openai.Image.create(\n",
    "  prompt=\"a really fat capybara\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response['data'][0]['url']\n",
    "image_url"
   ],
   "metadata": {
    "id": "5qQM_L-wsK8o",
    "ExecuteTime": {
     "end_time": "2023-12-14T09:00:30.665997Z",
     "start_time": "2023-12-14T09:00:22.406653400Z"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-LQTlKf4xJJRjJoDiPvPOzZJD/user-7LVzkZumq7golysoEl3KIfYo/img-L9bcoG0yMUlwg828DdZJrvPh.png?st=2023-12-14T08%3A00%3A32Z&se=2023-12-14T10%3A00%3A32Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-13T21%3A38%3A00Z&ske=2023-12-14T21%3A38%3A00Z&sks=b&skv=2021-08-06&sig=ChK2AFKOR341z80kClOr1%2BzAtUhHZCtYIlIjEz4TC/A%3D'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Show image\n",
    "Image(url=image_url)"
   ],
   "metadata": {
    "id": "-8NybqKNsPDC",
    "ExecuteTime": {
     "end_time": "2023-12-14T09:05:06.981650700Z",
     "start_time": "2023-12-14T09:05:06.851629900Z"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/html": "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-LQTlKf4xJJRjJoDiPvPOzZJD/user-7LVzkZumq7golysoEl3KIfYo/img-L9bcoG0yMUlwg828DdZJrvPh.png?st=2023-12-14T08%3A00%3A32Z&se=2023-12-14T10%3A00%3A32Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-13T21%3A38%3A00Z&ske=2023-12-14T21%3A38%3A00Z&sks=b&skv=2021-08-06&sig=ChK2AFKOR341z80kClOr1%2BzAtUhHZCtYIlIjEz4TC/A%3D\"/>",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 Création d'une variante d'une image\n",
    "\n",
    "DALL-E peut également être utilisé pour créer des variantes d'une image. Pour ce faire, il suffit de télécharger une image et de la soumettre à DALL-E. Le modèle génère alors une variante de l'image."
   ],
   "metadata": {
    "id": "jWM0DYmSs5rh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Download image from url\n",
    "def doanload_image(url_img, img_name):\n",
    "    img = requests.get(url_img).content\n",
    "    with open(img_name, 'wb') as handler:\n",
    "        handler.write(img)\n",
    "\n",
    "# Download image\n",
    "doanload_image(image_url, 'img.png')\n",
    "\n",
    "# Show image\n",
    "Image(url='img.png')"
   ],
   "metadata": {
    "id": "ETiQT2uwsR0b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create image variation\n",
    "response = openai.Image.create_variation(\n",
    "  image=open(\"img.png\", \"rb\"),\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response['data'][0]['url']\n",
    "\n",
    "Image(url=image_url)"
   ],
   "metadata": {
    "id": "sqv_Xfb7sbUZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Edition d'images à partir de texte et d'un masque\n",
    "\n",
    "DALL-E utilise ces informations pour générer une image éditée qui répond aux spécifications de l'utilisateur. Le modèle combine les informations du texte et du masque pour effectuer les modifications souhaitées sur l'image source.\n",
    "\n",
    "L'édition d'une image peut se faire à partir de la plateforme DALL-E : https://labs.openai.com/"
   ],
   "metadata": {
    "id": "bVFfxEAXsr-J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Edit image from text and mask\n",
    "response = openai.Image.create_edit(\n",
    "  image=open(\"img.png\", \"rb\"),\n",
    "  mask=open(\"mask.png\", \"rb\"),\n",
    "  prompt=\"red eyes\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response['data'][0]['url']\n",
    "\n",
    "Image(url=image_url)"
   ],
   "metadata": {
    "id": "Y6GijefWsW1t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8o2P7bPhwJ-O"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "UyGbQs9lwJ7X"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "G_6XoNbnwJ4w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "EzTCBdT1wJ2R"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_TaPfYRZwJzk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fD_r6DHwwJuF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fT5NJ-0ewJrV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "IUL9Mo50wKyD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xNCGIDuMwKvu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "lpbSUbDswJo6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "# Exercice\n",
    "\n",
    "### 1. Créez une classe **TextProcessor** qui possède les  méthodes suivantes :\n",
    "- `openai_translate` : qui traduit le texte passé en entré\n",
    "- `oepenai_text_sumary` : qui résume le texte passé en entrée\n",
    "- `openai_text_generator` : qui génère du texte en fonction d’une thématique et d’un contenu.\n",
    "- `openai_codex` : qui corrige le code passé en entré.\n",
    "- `openai_image` : qui retourne une image en fonction du prompte passé en argument.\n",
    "\n",
    "\n",
    "### 2. Concevez une page web qui reprend les différentes fonctionnalités créées précédemment sous forme de chatbot, les fonctionnalités s'activent en fonction des codes envoyés dans la conversation :\n",
    "\n",
    "**Fonctionnalités standards** :\n",
    "- `/translate + texte` : Traduit le text envoyé.\n",
    "- `/summary + texte` : Résume le texte envoyé.\n",
    "- `/imagine + prompt` : Génère une image à partir du primpt envoyé.\n",
    "- `/code + text` : Retourne le code corrigé.\n",
    "\n",
    "**Fonctionnalités Scraping** :\n",
    "- `/actu + theme` : Fait un résumé de 5 actualités à partir du theme envoyé.\n",
    "- `/json + url` : Retourne le contenu de l'url envoyé sous forme de json. (Utilisez le site [20minutes](https://www.20minutes.fr) pour faire vos tests).\n",
    "\n",
    "Le chat doit garder l'historique de la conversation.\n",
    "\n",
    "\n",
    "Vous pouvez également vous aider de la documentation : [Streamlit](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)"
   ],
   "metadata": {
    "id": "ty71uzBd53Ya"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.20minutes.fr/search?q=IA#gsc.tab=0&gsc.q=IA&gsc.page=1\").text\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "text = soup.text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(' ', '')\n",
    "\n",
    "text"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "aMLIGljREW0i",
    "outputId": "918dc424-fbe0-44d0-9161-e6a03c2d3ad3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Recherche-20MinutesRetoursurlapaged'accueil20MinutesMenuRechercheOkActualitéActualitéSociétéMondeFaitsdiversEconomieSantéPolitiqueJusticeFakeOffAUDIENCE«\\xa0T’asquoiàcacher\\xa0?\\xa0»Face-à-facetenduentreMoniqueOlivieretsonfil…états-unisHunterBidendéfielesrépublicainsetdéfendsonpèreRécapBidenrencontredesfamillesd’otagesaméricainsau68ejourduconflitRégionsRégionsParisMarseilleToulouseLilleNantesLyonBordeauxMontpellierArgumentsLesJOcontrairesàlaluttepourleclimat\\xa0?AnneHidalgorépondprocèsEllecroisedansBordeauxl’hommequ’elleaccusedeviolFaitdiversUneballeperduefinitdansletéléviseurd’unlogementenEssonneSportSportFootballBiathlonTennisRugbyBasketHandballCyclismeFormule1FOOTBALLNicolasSarkozyditn’avoireu«\\xa0aucunrôle\\xa0»danslaprolongationdeMbapp…BIATHLONCommesuivrel'étapedelaCoupedumondedeBiathlonàLenzerheideATHLETISMEKevinMayerreporteàmarssatentativedequalificationolympiqueDivertissementDivertissementCultureT'asvu?PeopleWebTélévisionCinémaSérieLivresCapitalePourquoiBourgesest-elledésignéecapitaleeuropéennedelacultureen\\xa0202…01:43REGARDDEPHOTOGRAPHEAïe…«\\xa0Legestepastrèstechnique\\xa0»deJonathanGraditcontreleFCSévill…01:09FILDESSTARSLeprinceWilliametKatherineSchwarzeneggersontdansleFildesStarsPlanètePlanèteÉcologieÉnergieAnimauxMétéoPlantesOcéansDANGERQuelssontlesrisquespourlevillagedeMaurienneaprèsl’éboulementXXL\\xa0…INONDATIONSDenombreuxsecteursinondésdansplusieursdépartementsduSud-OuestmystèreMaisquiamassacrélesmagnoliasd’uneruedeColomiers\\xa0?TempoTempoFoodStyleBien-êtreEscapadeModeCardigan,béret,pullsansmanches,le«\\xa0Papycool\\xa0»estderetourFROMAGEFONDUCommentréussiràcoupsûrsaracletteraclettepartyTroisastucesdiététiquespoursefaireplaisiraveclaracletteHigh-TechHigh-TechSciencesEspaceJeuxvidéoRéseauxsociauxBytheWebAmazonGoogleMétaflippantTeslarappelledesvoiturespourunrisqueliéàlaconduiteassistéeABONNEMENTFindel’abonnement«\\xa0Essentiel\\xa0»,premièreoffresanspublicitédeNetflixVOTREVIE,VOTREAVISCommentferiez-voussansWhatsApp\\xa0?CequedisentnoslecteursVieProVieProFichesmétiersViedebureauReconversionprofessionnelleEntrepreneuriatEMPLOIBienpréparersonentretienannueld’évaluationMAILSCommentéduquerlesgensquicliquenttoutletempssur«\\xa0répondreàtous\\xa0»…ENTREPRENEURIATMicroentrepreneur,quelssontvosdroitsàlaretraite\\xa0?JO2024JO2024AthlétismeBMXEscrimeGymnastiqueJudoMarathonNatationSkateArgumentsLesJOcontrairesàlaluttepourleclimat\\xa0?AnneHidalgorépondATHLETISMEKevinMayerreporteàmarssatentativedequalificationolympiqueRécap'ZelenskyfaceauCongrèsaméricainau657ejourdelaguerreenUkraineVidéosVidéos20MinutesTVBriefTéléTikTechLechoixdeCaroCemomentoùPleinlesdoigtsLeDebriefOhMyFake03:17archéologieVasteprojetderénovationdesmursd’osdanslesCatacombesdeParis01:17podiumAmandinePellissarddansletopdespersonnalitéslesplustapéessurGoogl…03:00MEEEuhLebruitdesclochesdesvachesdérangecesSuissesPodcastsPodcastsMinutePapillon!SixièmescienceToutSexpliqueJusteundroitL'étédansvosoreillesEnImagesEnImagesGuided'achatGuided'achatBonsplansHightechMaisonLoisirsCodespromoLotoRésultatsLotoFDJ:quelssontlesnumérosgagnantsdecemercredi13décem…bonplan7appareilsindispensablesàvotrecuisineavantlesfêtesdefind'annéebonplanDécouvrezlesoffresdefind’annéeSamsungLedirectLedirectLes+lusLes+partagésLes+commentésJeuxJeuxLemotdujourMotsFléchésMotsCroisésSudokuSudokuKillerSudokuirrégulierSolitairePuzzleServicesServicesJournalnumériqueMinutesMaisonSportuneBébésetMamansJournaldesSeniorsCuisined'AnnieMesNoticesInfostraficActualitéRégionsSportDivertissementPlanèteTempoHigh-TechVieProJO2024VidéosPodcastsEnImagesGuided'achatLedirectJeuxServicesNewslettersJournalJeuxHoroscopeDansl’actuGuerreHamas-IsraëlGuerreenUkraineLedirectActualitéActualitéSociétéMondeFaitsdiversEconomieSantéPolitiqueJusticeFakeOffLesenquêtesde20MinutesRégionsRégionsParisMarseilleToulouseLilleNantesLyonBordeauxMontpellierNiceRennesStrasbourgSportSportFootballBiathlonTennisRugbyBasketHandballCyclismeFormule1MercatoLivescoreDivertissementDivertissementCultureT'asvu?PeopleWebTélévisionCinémaSérieLivresMédiasMusiquePlanètePlanèteÉcologieÉnergieAnimauxMétéoPlantesOcéansTempoTempoFoodStyleBien-êtreEscapadeHigh-TechHigh-TechSciencesEspaceJeuxvidéoRéseauxsociauxBytheWebAmazonGoogleMétaApple20MintVieProVieProFichesmétiersViedebureauReconversionprofessionnelleEntrepreneuriatJO2024JO2024AthlétismeBMXEscrimeGymnastiqueJudoMarathonNatationSkateParalympiquesVidéosVidéos20MinutesTVBriefTéléTikTechLechoixdeCaroCemomentoùPleinlesdoigtsLeDebriefOhMyFakeLeBriefÇadatepasd'hierAnimaux2.0ExplicagifPodcastsPodcastsMinutePapillon!SixièmescienceToutSexpliqueJusteundroitL'étédansvosoreillesEnImagesGuided'achatGuided'achatBonsplansHightechMaisonLoisirsCodespromoLedirectLedirectLes+lusLes+partagésLes+commentésJeuxJeuxLemotdujourMotsFléchésMotsCroisésSudokuSudokuKillerSudokuirrégulierSolitairePuzzleMotsCoupésAnagrammesMotsMélangésMotsCasésiFléchésMotscodésJeudes8différencesRondedesMotsTakuzuKemaruFubukiBuzzKakuroGaramMatoku421BatailleNavaleHashiCryptoCampingServicesServicesJournalnumériqueMinutesMaisonSportuneBébésetMamansJournaldesSeniorsCuisined'AnnieMesNoticesInfostraficConjugaisonHoroscopeInfolocaleHoroscopechinoisMagazineRetoursurlapaged'accueil20MinutesRecherchePrécédentSuivantRechercheLedirectLes+lus21H56•AUDIENCE«\\xa0T’asquoiàcacher\\xa0?\\xa0»Face-à-facetenduentreMoniqueOlivieretsonfils21H33•CapitalePourquoiBourgesest-elledésignéecapitaleeuropéennedelacultureen\\xa02028\\xa0?20H26•états-unisHunterBidendéfielesrépublicainsetdéfendsonpère20H17•RécapBidenrencontredesfamillesd’otagesaméricainsau68ejourduconflit20H09•DANGERQuelssontlesrisquespourlevillagedeMaurienneaprèsl’éboulementXXL\\xa0?1TAPISPOURTOUSSouriresetflashsautourdeMiladypour«\\xa0LesTroisMousquetaires\\xa0»2tractationsZelenskyrepartdeWashingtonlesmainspresquevides3ACCIDENTDELAROUTEIlsetueauvolantd’unePorsche,sonamiquilesuivaitprendlafuiteRetoursurlapaged'accueil20MinutesFacebookTwitterInstagramSnapchatTiktokYoutubeDailymotionFlipboardNosapplicationsDécouvrezl‘ensembledenosapplications20Minutes!TéléchargersurAppStoreTéléchargersurGooglePlayActualitésSociétéFaitsdiversPolitiqueMondeSantéJusticeFactCheckingLespluspartagésServicesDistributionHoroscopeConjugaisonCuisined'AnnieMinutesMaisonBébésetMamansJournaldesSeniorsSportuneMesnotices20MinutesRecrutement20MinutesMédiaPaneldesLecteurs20MinutesLivresChartecommentaireJeuxJeuxenligneMotsfléchésSolitaireMotscroisésPuzzleMotscoupésSudokuAnagrammesShoppingGuided'achatHighTechBonsplansLotoFDJSmartphonesSoldesBlackFridayFrenchDaysCodespromoCodepromogratuitCodepromoAliExpressCodepromoCarrefourCodepromoDellCodepromoVistaprintCodepromoSheinCodepromoSFRCodepromoUberEatsPlandusiteNotrecharteJournalPDFArchivesPublicitéMentionslégalesCGUProtectiondesdonnéespersonnellesGestiondescookiesSupprimerlescookiespublicitaireséditeurNouscontacterChoixdeconsentement©Copyright20MinutesLafréquentationde20Minutesestcertifiéeparl‘ACPM\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 39
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "s6r8bs5nqkKJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "PkkoeO0cqkGx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "dWPjaab-qkDp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "3yB04JoNqkAx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6fwQVyZrqj9q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "qgBPuK8Zqj6i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "PXjvErobqj3r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "O_JYxqxAqj0q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "tb8Dwoxwqjxr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "SNcc_d0Tqjum"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Fine-tuning**\n",
    "\n",
    "Le fine-tuning permet d'entraîner un large modèle de langue tels que GPT3.5 sur des données spécifiques.\n",
    "\n",
    "\n",
    "Le réglage fin améliore l'apprentissage en quelques étapes en s'entraînant sur beaucoup plus d'exemples que ne peut en contenir l'invite, vous permettant d'obtenir de meilleurs résultats sur un grand nombre de tâches. Une fois qu'un modèle a été affiné, vous n'aurez plus besoin de fournir autant d'exemples dans l'invite. Cela permet de réduire les coûts et de permettre des requêtes à faible latence.\n",
    "\n",
    "Le réglage fin vous permet de tirer le meilleur parti des modèles disponibles via l'API en fournissant :\n",
    "\n",
    "1. Des résultats de meilleure qualité que les invites\n",
    "2. Capacité à s'entraîner sur plus d'exemples que ne peut en contenir une invite\n",
    "3. Économies de jetons grâce à des invites plus courtes\n",
    "4. Demandes de latence réduites\n",
    "\n",
    "\n",
    "<img src='https://images.openai.com/blob/d8d965fd-e54c-4c9b-9d7d-b5d9910d38b2/gpt-3-5-turbo-fine-tuning-and-api-updates.png?trim=0,0,0,0&width=3200'>\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "vNEdw_ai6HaU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Les modèles supportés par le fine-tuning**\n",
    "\n",
    "Un réglage fin est actuellement disponible pour les modèles suivants :\n",
    "\n",
    "- `gpt-3.5-turbo-0613`(recommandé)\n",
    "- `babbage-002`\n",
    "- `davinci-002`\n"
   ],
   "metadata": {
    "id": "HZZLTFQU6NYV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tarification**\n",
    "\n",
    "Les coûts de réglage fin sont répartis en deux catégories : le coût de formation initial et le coût d'utilisation :\n",
    "\n",
    "- Formation : 0,008 $ / 1 000 tokens\n",
    "\n",
    "- Entrée d'utilisation : 0,012 $ / 1 000 tokens\n",
    "\n",
    "- Sortie d'utilisation : 0,016 $ / 1 000 tokens\n",
    "\n",
    "Par exemple, un gpt-3.5-turbotravail de mise au point avec un fichier de formation de 100 000 jetons formés pendant 3 époques aurait un coût attendu de 2,40 $.\n",
    "\n"
   ],
   "metadata": {
    "id": "ZrnJPzeM6RMG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Cas d'utilisation courants**\n",
    "\n",
    "Quelques cas d'utilisation courants dans lesquels un réglage fin peut améliorer les résultats :\n",
    "\n",
    "- Définir le style, le ton, le format ou d'autres aspects qualitatifs\n",
    "- Améliorer la fiabilité pour produire le résultat souhaité\n",
    "- Correction des échecs de suivi d'invites complexes\n",
    "- Gérer de nombreux cas extrêmes de manière spécifique\n",
    "- Exécuter une nouvelle compétence ou une nouvelle tâche difficile à articuler dans une invite\n",
    "\n",
    "Une manière générale d'envisager ces cas consiste à considérer qu'il est plus facile de « montrer, pas de dire ». Dans les sections à venir, nous explorerons comment configurer les données pour un réglage fin et divers exemples dans lesquels le réglage fin améliore les performances par rapport au modèle de base."
   ],
   "metadata": {
    "id": "hXcIYce46TvY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Méthodologie**\n",
    "\n",
    "Le réglage fin améliore l'apprentissage en quelques étapes en s'entraînant sur beaucoup plus d'exemples que ne peut en contenir l'invite, vous permettant d'obtenir de meilleurs résultats sur un grand nombre de tâches. Une fois qu'un modèle a été affiné, vous n'aurez plus besoin de fournir autant d'exemples dans l'invite. Cela permet de réduire les coûts et de permettre des requêtes à faible latence.\n",
    "\n",
    "À un niveau élevé, le réglage fin implique les étapes suivantes :\n",
    "\n",
    "1. Préparer et télécharger les données d'entraînement\n",
    "2. Former un nouveau modèle affiné\n",
    "3. Utilisez votre modèle affiné\n"
   ],
   "metadata": {
    "id": "03tZTidi6WRn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1. Préparer et télécharger les données d'entraînement**\n",
    "\n",
    "Le réglage fin nécessite un fichier de formation au format JSONL. Chaque ligne du fichier doit contenir un objet JSON avec les clés/valeurs suivantes :\n",
    "\n",
    "- Pour les modèles `ChatCompletion` gt3.5-turbo-0613 :\n",
    "\n",
    "```{\n",
    "    \"message\":[\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"contenu du message\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"contenu du message\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\":\"contenu du message\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "- Pour les modèles `TextGeneration` :\n",
    "```\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```\n",
    "\n",
    "`**Attention**` : **Chaque message doit être placé sur une ligne dans le fichier JSONL. Les messages peuvent être de n'importe quelle longueur.**\n",
    "\n",
    "**Au moins 10 messages sont nécessaires** pour entraîner un modèle. Pour obtenir de meilleurs résultats, nous recommandons d'utiliser au moins 100 messages. Pour les modèles de génération de texte, nous recommandons d'utiliser au moins 1000 messages.\n",
    "\n",
    "Le réglage fin est disponible à partir de la version 0.28.0 de l'[API d'OpenAI](https://platform.openai.com/docs/guides/fine-tuning)."
   ],
   "metadata": {
    "id": "dZD1WxVU6Yt5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Structure des données pour l'entrainement d'un modèle ChatCompletion\n",
    "# Moèle concerné : gpt-3.5-turbo-0613 :\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What is fine-tuning in machine learning?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning in machine learning involves training a pre-existing model on a specific task or dataset to make it more accurate or suitable for that task. It's like giving the model specialized knowledge.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Why is fine-tuning important for AI models?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning is crucial because it allows AI models to adapt to specific applications or domains, improving their performance and making them more useful in real-world scenarios.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Can you explain the steps involved in fine-tuning a language model?\"}, {\"role\": \"assistant\", \"content\": \"Certainly! Fine-tuning typically includes preparing training data, training the model on that data, and then using the fine-tuned model for your specific tasks. It's a process of specialization.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What models can be fine-tuned for language tasks?\"}, {\"role\": \"assistant\", \"content\": \"Currently, you can fine-tune models like gpt-3.5-turbo-0613, babbage-002, and davinci-002. These models can be adapted to various language-related tasks.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"When should I consider using fine-tuning for AI models?\"}, {\"role\": \"assistant\", \"content\": \"You should consider fine-tuning when you need to tailor a model for specific tasks or improve its performance on certain applications. It's a powerful tool for customization.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How can fine-tuning benefit natural language processing (NLP) tasks?\"}, {\"role\": \"assistant\", \"content\": \"Fine-tuning can make NLP models more accurate and contextually aware for tasks like text generation, translation, sentiment analysis, and more. It's all about making the model 'smarter' in a particular domain.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What is the role of training data in fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"Training data is essential for fine-tuning as it helps the model learn from examples. You need a diverse set of conversations or text that reflects the scenarios you want the model to handle.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How many training examples should I provide for fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"You should provide at least 10 training examples, but for optimal results, it's often recommended to have between 50 and 100 examples. The right number depends on your specific use case.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What are the token limits for fine-tuning examples?\"}, {\"role\": \"assistant\", \"content\": \"Each fine-tuning example is limited to 4096 tokens. Make sure the total token count in the messages' content is under 4,000 to fit within this limit.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How can I estimate the costs of fine-tuning?\"}, {\"role\": \"assistant\", \"content\": \"You can estimate the costs by using this formula: base cost per 1,000 tokens * number of tokens in the input file * number of training epochs. This will give you an idea of the expected cost for your fine-tuning job.\"}]}\n",
    "\n",
    "\n",
    "# Structure des données pour l'entrainement d'un modèle Completion\n",
    "# Moèle concerné : babbage-002 et davinci-002\n",
    "\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
   ],
   "metadata": {
    "id": "71dCavKG6H7a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2. Former un nouveau modèle affiné**\n",
    "\n",
    "Une fois les instructions de formation préparées et placées dans un fichier `data.jsonl`, vous pouvez commencer à former votre modèle affiné en préparant les données d'entrainement puis en lançant la formation."
   ],
   "metadata": {
    "id": "RczmCk1P6jHP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "# Préparation des données pour l'entrainement d'un modèle ChatCompletion\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "response = openai.File.create(\n",
    "  file=open(\"./data.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "response"
   ],
   "metadata": {
    "id": "u1ViCFq-6jm6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Entrainement du modèle ChatCompletion sur les serveurs d'OpenAI\n",
    "response_train_model = openai.FineTuningJob.create(\n",
    "    training_file=response['id'],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "response_train_model"
   ],
   "metadata": {
    "id": "tYfo6FOf6oGN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "openai.FineTuningJob.list()"
   ],
   "metadata": {
    "id": "H754ewbi6p9f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **3. Utilisez votre modèle affiné**\n",
    "\n",
    "Une fois que votre modèle est formé, vous pouvez l'utiliser pour générer des résultats en utilisant l'API comme vous le feriez avec n'importe quel autre modèle. Il est possible de visualiser la courbe d'apprentissage de votre modèle dans le tableau de bord de votre compte :\n",
    "https://platform.openai.com/finetune"
   ],
   "metadata": {
    "id": "-54aZADz6mDC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "[model['id'] for model in openai.Model().list()[\"data\"]]"
   ],
   "metadata": {
    "id": "kTw9t3p76vfJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Utilisation du modèle fine-tuné\n",
    "reponse = openai.ChatCompletion.create(\n",
    "    model='ft:gpt-3.5-turbo-0613:dawan::81DNLWeI',\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\":\"What is fine-tuning in machine learning?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "reponse['choices'][0]['message'][\"content\"]"
   ],
   "metadata": {
    "id": "QhgM41x66yIA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "\n",
    "# Exercice\n",
    "\n",
    "1. A L'aide de ChatGPT, créez un fichier de données d'entrainement au format JSONL contenant au moins 10 messages puis placez le dans un fichier `data.jsonl` Entrainez ensuite un modèle de chatbot sur un domaine de votre choix.\n",
    "**Astuce** : Copiez le contenu de votre choix (exemple : une page internet) puis demandez à GhatGPT de générer le fichier de données d'entrainement au format JSONL.\n",
    "\n",
    "2. Créez une méthode `openai_chat_finetune()` qui prend en paramètre le nom du fichier de données d'entrainement et retourne le nom du modèle entrainé.\n",
    "\n",
    "### **Exemple de prompt à envoyer à ChatGPT**\n",
    "\n",
    "```\n",
    "\n",
    "A partir du document suivant crée-moi un fichier jsonl contentant des exemples d'échange entre un chatbot et un utilisateur qui sera utilisé pour le fine-tuning d'un model gpt3 pour répondre aux questions de développeurs qui souhaite comprendre comment fonctionne le finetuning. Le jsonl aura la forme suivante :\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"definir le rol system\"}, {\"role\": \"user\", \"content\": \"question type posé par un utilisateur\"}, {\"role\": \"assistant\", \"content\": \"Réponse de l'assistant\"}]}\n",
    "\n",
    "\n",
    "Le modèle du fichier json doit avoir cette forme :\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "\n",
    "\n",
    "Propose moi à nouveau 20 exemples d'intéraction entre un assistant et un utilisateur au sujet du finetuning en respectant le modèle donné.\n",
    "\n",
    "Voici la documentation à partir de laquelle tu dois créer les exemples : à compléter\n",
    "\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "GlCEJmOv6712"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "kaS7n6bl68GU"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
